--- a/llama.cpp
+++ b/llama.cpp
@@ -9958,8 +9958,10 @@ struct llama_context * llama_new_context_with_model(
             LLAMA_LOG_INFO("%s: VRAM scratch buffer: %.2f MB\n", __func__, alloc_size / 1024.0 / 1024.0);

             // calculate total VRAM usage
+            // FIX: Add NULL check to prevent segfault on sparse inference models
             auto add_tensor = [](const ggml_tensor * t, size_t & size) {
-                if (t->backend == GGML_BACKEND_GPU || t->backend == GGML_BACKEND_GPU_SPLIT) {
+                if (t != nullptr &&
+                    (t->backend == GGML_BACKEND_GPU || t->backend == GGML_BACKEND_GPU_SPLIT)) {
                     size += ggml_nbytes(t);
                 }
             };
--- a/llama.cpp
+++ b/llama.cpp
@@ -4557,14 +4557,15 @@ static struct ggml_tensor * llm_build_sparse_mul_mat(
     std::string full_name = "ffn_" + std::string(name) + "_sparse";
     ggml_tensor * out = nullptr;

-#ifdef GGML_USE_HIPBLAS
-// WARNING: THIS IS A HACK!
-// if up_gpu->data is null
-// inference fails when model exceeds 40B on rocm device
-// so we just let up_gpu->data point to itself
-
-    up_gpu->data = up_gpu;
-
+#if defined(GGML_USE_HIPBLAS) || defined(GGML_USE_CUBLAS)
+    // FIX: Properly handle NULL up_gpu for gfx1151 and other ROCm targets
+    // Instead of the dangerous self-pointer hack, skip GPU path when up_gpu is NULL
+    if (up_gpu == nullptr) {
+        // Fall through to CPU-only path
+        out = ggml_mul_mat_idx(ctx, up, inp, idx, gpu_index);
+        cb(out, full_name.c_str());
+        return out;
+    }
 #endif

 #ifdef GGML_USE_CUBLAS
@@ -4610,14 +4611,15 @@ static struct ggml_tensor * llm_build_sparse_axpy(
     std::string full_name = "ffn_" + std::string(name) + "_sparse";
     ggml_tensor * out = nullptr;

-#ifdef GGML_USE_HIPBLAS
-// WARNING: THIS IS A HACK!
-// if wt_gpu->data is null
-// inference fails when model exceeds 40B on rocm device
-// so we just let wt_gpu->data point to itself
-
-    wt_gpu->data = wt_gpu;
-
+#if defined(GGML_USE_HIPBLAS) || defined(GGML_USE_CUBLAS)
+    // FIX: Properly handle NULL wt_gpu for gfx1151 and other ROCm targets
+    // Instead of the dangerous self-pointer hack, skip GPU path when wt_gpu is NULL
+    if (wt_gpu == nullptr) {
+        // Fall through to CPU-only path
+        out = ggml_axpy(ctx, w_t, x, sparse_idx, gpu_index);
+        cb(out, full_name.c_str());
+        return out;
+    }
 #endif

 #ifdef GGML_USE_CUBLAS
--- a/ggml-cuda.cu
+++ b/ggml-cuda.cu
@@ -9175,6 +9175,9 @@ void ggml_cuda_assign_buffers_no_alloc(struct ggml_tensor * tensor) {
 }

 void ggml_cuda_assign_buffers_no_scratch(struct ggml_tensor * tensor) {
+    // FIX: Add NULL check like ggml_cuda_assign_buffers() has
+    if (tensor == NULL)
+        return;
     ggml_cuda_assign_buffers_impl(tensor, false, false, false);
 }

--- a/llama.cpp
+++ b/llama.cpp
@@ -1646,6 +1646,14 @@ static bool llama_kv_cache_init(

     cache.k = ggml_new_tensor_1d(cache.ctx, wtype, n_elements);
     cache.v = ggml_new_tensor_1d(cache.ctx, wtype, n_elements);
+
+    // FIX: Check for allocation failure before using tensors
+    if (cache.k == nullptr || cache.v == nullptr) {
+        LLAMA_LOG_ERROR("%s: failed to allocate KV cache tensors\n", __func__);
+        ggml_free(cache.ctx);
+        return false;
+    }
+
     ggml_set_name(cache.k, "cache_k");
     ggml_set_name(cache.v, "cache_v");

